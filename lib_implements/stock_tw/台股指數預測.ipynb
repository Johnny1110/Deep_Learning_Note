{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>指數</th>\n",
       "      <th>漲跌</th>\n",
       "      <th>動盪</th>\n",
       "      <th>交易量</th>\n",
       "      <th>融資</th>\n",
       "      <th>融券</th>\n",
       "      <th>自營商</th>\n",
       "      <th>投信</th>\n",
       "      <th>外資</th>\n",
       "      <th>3日</th>\n",
       "      <th>...</th>\n",
       "      <th>14日</th>\n",
       "      <th>15日</th>\n",
       "      <th>16日</th>\n",
       "      <th>17日</th>\n",
       "      <th>18日</th>\n",
       "      <th>19日</th>\n",
       "      <th>20日</th>\n",
       "      <th>60日</th>\n",
       "      <th>target_1</th>\n",
       "      <th>target_2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>日期</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2004/7/1</th>\n",
       "      <td>5836.91</td>\n",
       "      <td>-2.53</td>\n",
       "      <td>59.71</td>\n",
       "      <td>692.00694</td>\n",
       "      <td>206.16345</td>\n",
       "      <td>84.9099</td>\n",
       "      <td>7.329094</td>\n",
       "      <td>-0.221718</td>\n",
       "      <td>9.635603</td>\n",
       "      <td>30.95333</td>\n",
       "      <td>...</td>\n",
       "      <td>140.86500</td>\n",
       "      <td>129.43400</td>\n",
       "      <td>113.29500</td>\n",
       "      <td>97.81588</td>\n",
       "      <td>86.88667</td>\n",
       "      <td>88.20947</td>\n",
       "      <td>92.0720</td>\n",
       "      <td>-238.52617</td>\n",
       "      <td>-103.34</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004/7/2</th>\n",
       "      <td>5746.70</td>\n",
       "      <td>-90.21</td>\n",
       "      <td>58.99</td>\n",
       "      <td>531.13441</td>\n",
       "      <td>206.57564</td>\n",
       "      <td>78.7084</td>\n",
       "      <td>-13.862259</td>\n",
       "      <td>-16.050376</td>\n",
       "      <td>-82.260722</td>\n",
       "      <td>-60.98333</td>\n",
       "      <td>...</td>\n",
       "      <td>49.82429</td>\n",
       "      <td>47.27800</td>\n",
       "      <td>36.77250</td>\n",
       "      <td>21.72706</td>\n",
       "      <td>7.18333</td>\n",
       "      <td>-3.14842</td>\n",
       "      <td>-1.9005</td>\n",
       "      <td>-313.30017</td>\n",
       "      <td>-18.92</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004/7/5</th>\n",
       "      <td>5659.78</td>\n",
       "      <td>-86.92</td>\n",
       "      <td>79.36</td>\n",
       "      <td>440.94036</td>\n",
       "      <td>205.87461</td>\n",
       "      <td>72.0990</td>\n",
       "      <td>-14.960335</td>\n",
       "      <td>-19.800116</td>\n",
       "      <td>-16.629336</td>\n",
       "      <td>-88.01667</td>\n",
       "      <td>...</td>\n",
       "      <td>-43.21714</td>\n",
       "      <td>-34.62267</td>\n",
       "      <td>-37.16438</td>\n",
       "      <td>-47.19765</td>\n",
       "      <td>-61.57111</td>\n",
       "      <td>-75.54000</td>\n",
       "      <td>-85.5650</td>\n",
       "      <td>-384.21050</td>\n",
       "      <td>53.61</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004/7/6</th>\n",
       "      <td>5733.57</td>\n",
       "      <td>73.79</td>\n",
       "      <td>76.65</td>\n",
       "      <td>414.49502</td>\n",
       "      <td>205.70078</td>\n",
       "      <td>75.2901</td>\n",
       "      <td>3.310531</td>\n",
       "      <td>-7.703426</td>\n",
       "      <td>-0.348274</td>\n",
       "      <td>20.22000</td>\n",
       "      <td>...</td>\n",
       "      <td>24.35286</td>\n",
       "      <td>28.53467</td>\n",
       "      <td>36.71937</td>\n",
       "      <td>34.47118</td>\n",
       "      <td>25.11500</td>\n",
       "      <td>11.57579</td>\n",
       "      <td>-1.6625</td>\n",
       "      <td>-293.01700</td>\n",
       "      <td>44.15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004/7/7</th>\n",
       "      <td>5727.78</td>\n",
       "      <td>-5.79</td>\n",
       "      <td>101.45</td>\n",
       "      <td>503.79043</td>\n",
       "      <td>205.75792</td>\n",
       "      <td>71.5637</td>\n",
       "      <td>-4.501554</td>\n",
       "      <td>6.698410</td>\n",
       "      <td>-1.273157</td>\n",
       "      <td>20.73667</td>\n",
       "      <td>...</td>\n",
       "      <td>6.59000</td>\n",
       "      <td>17.32533</td>\n",
       "      <td>21.32312</td>\n",
       "      <td>29.11000</td>\n",
       "      <td>27.08778</td>\n",
       "      <td>18.30789</td>\n",
       "      <td>5.4965</td>\n",
       "      <td>-281.03117</td>\n",
       "      <td>30.96</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020/8/14</th>\n",
       "      <td>12795.46</td>\n",
       "      <td>32.33</td>\n",
       "      <td>122.12</td>\n",
       "      <td>2145.85977</td>\n",
       "      <td>72.41246</td>\n",
       "      <td>68.0235</td>\n",
       "      <td>4.332400</td>\n",
       "      <td>6.888281</td>\n",
       "      <td>39.489178</td>\n",
       "      <td>52.48000</td>\n",
       "      <td>...</td>\n",
       "      <td>67.87643</td>\n",
       "      <td>77.16200</td>\n",
       "      <td>103.05312</td>\n",
       "      <td>119.48647</td>\n",
       "      <td>130.74778</td>\n",
       "      <td>144.80895</td>\n",
       "      <td>168.6145</td>\n",
       "      <td>863.02700</td>\n",
       "      <td>-16.82</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020/8/17</th>\n",
       "      <td>12956.11</td>\n",
       "      <td>160.65</td>\n",
       "      <td>155.42</td>\n",
       "      <td>2228.58664</td>\n",
       "      <td>72.94443</td>\n",
       "      <td>68.3645</td>\n",
       "      <td>12.277874</td>\n",
       "      <td>7.280122</td>\n",
       "      <td>96.455224</td>\n",
       "      <td>117.87667</td>\n",
       "      <td>...</td>\n",
       "      <td>202.14214</td>\n",
       "      <td>213.29133</td>\n",
       "      <td>222.94875</td>\n",
       "      <td>248.19118</td>\n",
       "      <td>264.57333</td>\n",
       "      <td>276.06105</td>\n",
       "      <td>290.1860</td>\n",
       "      <td>991.21367</td>\n",
       "      <td>-593.47</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020/8/18</th>\n",
       "      <td>12872.14</td>\n",
       "      <td>-83.97</td>\n",
       "      <td>141.33</td>\n",
       "      <td>2504.51420</td>\n",
       "      <td>72.98284</td>\n",
       "      <td>68.5825</td>\n",
       "      <td>-5.175571</td>\n",
       "      <td>-0.132520</td>\n",
       "      <td>-29.954802</td>\n",
       "      <td>-2.43000</td>\n",
       "      <td>...</td>\n",
       "      <td>94.51714</td>\n",
       "      <td>110.29400</td>\n",
       "      <td>121.23875</td>\n",
       "      <td>130.80353</td>\n",
       "      <td>155.09778</td>\n",
       "      <td>171.09789</td>\n",
       "      <td>182.4865</td>\n",
       "      <td>872.89383</td>\n",
       "      <td>-264.30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020/8/19</th>\n",
       "      <td>12778.64</td>\n",
       "      <td>-93.50</td>\n",
       "      <td>171.47</td>\n",
       "      <td>2555.84456</td>\n",
       "      <td>73.11886</td>\n",
       "      <td>68.6890</td>\n",
       "      <td>-20.282008</td>\n",
       "      <td>-1.413083</td>\n",
       "      <td>-5.197372</td>\n",
       "      <td>-90.32333</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.96286</td>\n",
       "      <td>0.94933</td>\n",
       "      <td>15.74438</td>\n",
       "      <td>26.10706</td>\n",
       "      <td>35.23111</td>\n",
       "      <td>58.35579</td>\n",
       "      <td>73.7180</td>\n",
       "      <td>747.60283</td>\n",
       "      <td>-131.51</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020/8/20</th>\n",
       "      <td>12362.64</td>\n",
       "      <td>-416.00</td>\n",
       "      <td>619.99</td>\n",
       "      <td>3524.66832</td>\n",
       "      <td>71.60134</td>\n",
       "      <td>62.6071</td>\n",
       "      <td>-28.979593</td>\n",
       "      <td>-16.987857</td>\n",
       "      <td>-146.705447</td>\n",
       "      <td>-308.50000</td>\n",
       "      <td>...</td>\n",
       "      <td>-397.38000</td>\n",
       "      <td>-391.03200</td>\n",
       "      <td>-389.11000</td>\n",
       "      <td>-376.71118</td>\n",
       "      <td>-368.23222</td>\n",
       "      <td>-360.72842</td>\n",
       "      <td>-339.7620</td>\n",
       "      <td>308.84567</td>\n",
       "      <td>395.61</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3988 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 指數      漲跌      動盪         交易量         融資       融券  \\\n",
       "日期                                                                    \n",
       "2004/7/1    5836.91   -2.53   59.71   692.00694  206.16345  84.9099   \n",
       "2004/7/2    5746.70  -90.21   58.99   531.13441  206.57564  78.7084   \n",
       "2004/7/5    5659.78  -86.92   79.36   440.94036  205.87461  72.0990   \n",
       "2004/7/6    5733.57   73.79   76.65   414.49502  205.70078  75.2901   \n",
       "2004/7/7    5727.78   -5.79  101.45   503.79043  205.75792  71.5637   \n",
       "...             ...     ...     ...         ...        ...      ...   \n",
       "2020/8/14  12795.46   32.33  122.12  2145.85977   72.41246  68.0235   \n",
       "2020/8/17  12956.11  160.65  155.42  2228.58664   72.94443  68.3645   \n",
       "2020/8/18  12872.14  -83.97  141.33  2504.51420   72.98284  68.5825   \n",
       "2020/8/19  12778.64  -93.50  171.47  2555.84456   73.11886  68.6890   \n",
       "2020/8/20  12362.64 -416.00  619.99  3524.66832   71.60134  62.6071   \n",
       "\n",
       "                 自營商         投信          外資         3日  ...        14日  \\\n",
       "日期                                                      ...              \n",
       "2004/7/1    7.329094  -0.221718    9.635603   30.95333  ...  140.86500   \n",
       "2004/7/2  -13.862259 -16.050376  -82.260722  -60.98333  ...   49.82429   \n",
       "2004/7/5  -14.960335 -19.800116  -16.629336  -88.01667  ...  -43.21714   \n",
       "2004/7/6    3.310531  -7.703426   -0.348274   20.22000  ...   24.35286   \n",
       "2004/7/7   -4.501554   6.698410   -1.273157   20.73667  ...    6.59000   \n",
       "...              ...        ...         ...        ...  ...        ...   \n",
       "2020/8/14   4.332400   6.888281   39.489178   52.48000  ...   67.87643   \n",
       "2020/8/17  12.277874   7.280122   96.455224  117.87667  ...  202.14214   \n",
       "2020/8/18  -5.175571  -0.132520  -29.954802   -2.43000  ...   94.51714   \n",
       "2020/8/19 -20.282008  -1.413083   -5.197372  -90.32333  ...   -2.96286   \n",
       "2020/8/20 -28.979593 -16.987857 -146.705447 -308.50000  ... -397.38000   \n",
       "\n",
       "                 15日        16日        17日        18日        19日       20日  \\\n",
       "日期                                                                           \n",
       "2004/7/1   129.43400  113.29500   97.81588   86.88667   88.20947   92.0720   \n",
       "2004/7/2    47.27800   36.77250   21.72706    7.18333   -3.14842   -1.9005   \n",
       "2004/7/5   -34.62267  -37.16438  -47.19765  -61.57111  -75.54000  -85.5650   \n",
       "2004/7/6    28.53467   36.71937   34.47118   25.11500   11.57579   -1.6625   \n",
       "2004/7/7    17.32533   21.32312   29.11000   27.08778   18.30789    5.4965   \n",
       "...              ...        ...        ...        ...        ...       ...   \n",
       "2020/8/14   77.16200  103.05312  119.48647  130.74778  144.80895  168.6145   \n",
       "2020/8/17  213.29133  222.94875  248.19118  264.57333  276.06105  290.1860   \n",
       "2020/8/18  110.29400  121.23875  130.80353  155.09778  171.09789  182.4865   \n",
       "2020/8/19    0.94933   15.74438   26.10706   35.23111   58.35579   73.7180   \n",
       "2020/8/20 -391.03200 -389.11000 -376.71118 -368.23222 -360.72842 -339.7620   \n",
       "\n",
       "                 60日  target_1  target_2  \n",
       "日期                                        \n",
       "2004/7/1  -238.52617   -103.34         0  \n",
       "2004/7/2  -313.30017    -18.92         0  \n",
       "2004/7/5  -384.21050     53.61         1  \n",
       "2004/7/6  -293.01700     44.15         1  \n",
       "2004/7/7  -281.03117     30.96         1  \n",
       "...              ...       ...       ...  \n",
       "2020/8/14  863.02700    -16.82         0  \n",
       "2020/8/17  991.21367   -593.47         0  \n",
       "2020/8/18  872.89383   -264.30         0  \n",
       "2020/8/19  747.60283   -131.51         0  \n",
       "2020/8/20  308.84567    395.61         1  \n",
       "\n",
       "[3988 rows x 30 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_data = pd.read_csv(\"final_data.csv\", index_col=0)\n",
    "source_data.dropna(how='any',inplace=True) # 處理缺失直\n",
    "\n",
    "source_data = source_data.reindex(index=source_data.index[::-1])\n",
    "\n",
    "source_data\n",
    "# target_1 為未來三天的漲跌加總，target_2 為蔣總後的正負。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess(df, time_frame):\n",
    "    data_value = df.to_numpy()  # 將 data_frame 轉為  numpy array\n",
    "    \n",
    "    result = []\n",
    "    for index in range(len(data_value) - time_frame):\n",
    "        result.append(data_value[index: index + time_frame])\n",
    "    \n",
    "    result = np.array(result)\n",
    "    number_train = round(0.9 * result.shape[0]) # 90% 資料用來訓練\n",
    "    \n",
    "    # 訓練資料\n",
    "    x_train = result[:int(number_train), :, :-2]\n",
    "    y_train = result[:int(number_train), -1, -1]\n",
    "    # 測試資料\n",
    "    x_test = result[int(number_train):, :, :-2]\n",
    "    y_test = result[int(number_train):, -1, -1]\n",
    "    \n",
    "    return [x_train, y_train, x_test, y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = data_preprocess(source_data, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape:  (3571, 20, 28)\n",
      "y_train.shape:  (3571,)\n",
      "x_test.shape:  (397, 20, 28)\n",
      "y_test.shape:  (397,)\n"
     ]
    }
   ],
   "source": [
    "print(\"x_train.shape: \", x_train.shape)\n",
    "print(\"y_train.shape: \", y_train.shape)\n",
    "print(\"x_test.shape: \", x_test.shape)\n",
    "print(\"y_test.shape: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00  1.00000000e+00  1.00000000e+00\n",
      "    1.00000000e+00  1.00000000e+00  1.00000000e+00  1.00000000e+00]\n",
      "  [ 8.41470957e-01  4.95097876e-01  2.65063316e-01  1.38502866e-01\n",
      "    7.19064549e-02  3.72673050e-02  1.93057787e-02  9.99983307e-03\n",
      "    5.17945131e-03  2.68269260e-03  1.38949510e-03  7.19685631e-04\n",
      "    3.72759358e-04  1.93069776e-04  5.40302277e-01  8.68837237e-01\n",
      "    9.64231014e-01  9.90362048e-01  9.97411370e-01  9.99305308e-01\n",
      "    9.99813616e-01  9.99949992e-01  9.99986589e-01  9.99996424e-01\n",
      "    9.99999046e-01  9.99999762e-01  9.99999940e-01  1.00000000e+00]\n",
      "  [ 9.09297407e-01  8.60318899e-01  5.11164546e-01  2.74335951e-01\n",
      "    1.43440634e-01  7.44828358e-02  3.86043601e-02  1.99986659e-02\n",
      "    1.03587639e-02  5.36536565e-03  2.77898740e-03  1.43937080e-03\n",
      "    7.45518657e-04  3.86139523e-04 -4.16146845e-01  5.09756207e-01\n",
      "    8.59482884e-01  9.61633921e-01  9.89658952e-01  9.97222304e-01\n",
      "    9.99254584e-01  9.99800026e-01  9.99946356e-01  9.99985635e-01\n",
      "    9.99996126e-01  9.99998987e-01  9.99999702e-01  9.99999940e-01]\n",
      "  [ 1.41120002e-01  9.99856293e-01  7.20698059e-01  4.04880971e-01\n",
      "    2.14232191e-01  1.11594878e-01  5.78885525e-02  2.99955010e-02\n",
      "    1.55377984e-02  8.04800075e-03  4.16847458e-03  2.15905532e-03\n",
      "    1.11827790e-03  5.79209300e-04 -9.89992499e-01  1.69531107e-02\n",
      "    6.93249106e-01  9.14369404e-01  9.76782739e-01  9.93753791e-01\n",
      "    9.98323023e-01  9.99550045e-01  9.99879301e-01  9.99967635e-01\n",
      "    9.99991298e-01  9.99997675e-01  9.99999404e-01  9.99999821e-01]\n",
      "  [-7.56802499e-01  8.77105772e-01  8.78674328e-01  5.27621508e-01\n",
      "    2.83914626e-01  1.48551881e-01  7.71511644e-02  3.99893336e-02\n",
      "    2.07164157e-02  1.07305776e-02  5.55795338e-03  2.87873880e-03\n",
      "    1.49103696e-03  7.72278989e-04 -6.53643608e-01 -4.80297208e-01\n",
      "    4.77421671e-01  8.49479556e-01  9.58849549e-01  9.88904595e-01\n",
      "    9.97019410e-01  9.99200106e-01  9.99785364e-01  9.99942422e-01\n",
      "    9.99984562e-01  9.99995828e-01  9.99998868e-01  9.99999702e-01]\n",
      "  [-9.58924294e-01  5.24268031e-01  9.73792017e-01  6.40191674e-01\n",
      "    3.52127135e-01  1.85302496e-01  9.63850245e-02  4.99791689e-02\n",
      "    2.58944780e-02  1.34130763e-02  6.94742147e-03  3.59842065e-03\n",
      "    1.86379580e-03  9.65348736e-04  2.83662200e-01 -8.51553321e-01\n",
      "    2.27440447e-01  7.68215239e-01  9.35952187e-01  9.82681513e-01\n",
      "    9.95344102e-01  9.98750269e-01  9.99664664e-01  9.99910057e-01\n",
      "    9.99975860e-01  9.99993503e-01  9.99998271e-01  9.99999523e-01]\n",
      "  [-2.79415488e-01  3.39013487e-02  9.99246538e-01  7.40421534e-01\n",
      "    4.18516636e-01  2.21795663e-01  1.15582950e-01  5.99640049e-02\n",
      "    3.10718454e-02  1.60954800e-02  8.33687652e-03  4.31810040e-03\n",
      "    2.23655440e-03  1.15841837e-03  9.60170269e-01 -9.99425173e-01\n",
      "   -3.88114005e-02  6.72142804e-01  9.08209145e-01  9.75093186e-01\n",
      "    9.93297815e-01  9.98200536e-01  9.99517143e-01  9.99870479e-01\n",
      "    9.99965250e-01  9.99990702e-01  9.99997497e-01  9.99999344e-01]\n",
      "  [ 6.56986594e-01 -4.65358526e-01  9.53217030e-01  8.26379061e-01\n",
      "    4.82739329e-01  2.57980704e-01  1.34737790e-01  6.99428469e-02\n",
      "    3.62483785e-02  1.87777672e-02  9.72631481e-03  5.03777852e-03\n",
      "    2.60931253e-03  1.35148806e-03  7.53902256e-01 -8.85122299e-01\n",
      "   -3.02286744e-01  5.63114226e-01  8.75764072e-01  9.66150045e-01\n",
      "    9.90881264e-01  9.97551024e-01  9.99342799e-01  9.99823689e-01\n",
      "    9.99952674e-01  9.99987304e-01  9.99996603e-01  9.99999106e-01]\n",
      "  [ 9.89358246e-01 -8.42542946e-01  8.38996291e-01  8.96407425e-01\n",
      "    5.44462800e-01  2.93807298e-01  1.53842419e-01  7.99146965e-02\n",
      "    4.14239429e-02  2.14599185e-02  1.11157354e-02  5.75745339e-03\n",
      "    2.98207067e-03  1.54455751e-03 -1.45500034e-01 -5.38629174e-01\n",
      "   -5.44137120e-01  4.43231076e-01  8.38784993e-01  9.55864668e-01\n",
      "    9.88095403e-01  9.96801734e-01  9.99141634e-01  9.99769688e-01\n",
      "    9.99938190e-01  9.99983430e-01  9.99995530e-01  9.99998808e-01]\n",
      "  [ 4.12118495e-01 -9.98706818e-01  6.64755464e-01  9.49156642e-01\n",
      "    6.03367448e-01  3.29225689e-01  1.72889695e-01  8.98785517e-02\n",
      "    4.65983897e-02  2.41419170e-02  1.25051336e-02  6.47712592e-03\n",
      "    3.35482811e-03  1.73762708e-03 -9.11130250e-01 -5.08398414e-02\n",
      "   -7.47061014e-01  3.14804226e-01  7.97463298e-01  9.44251239e-01\n",
      "    9.84941185e-01  9.95952725e-01  9.98913705e-01  9.99708533e-01\n",
      "    9.99921799e-01  9.99979019e-01  9.99994397e-01  9.99998510e-01]\n",
      "  [-5.44021130e-01 -8.92884374e-01  4.42959368e-01  9.83609974e-01\n",
      "    6.59148335e-01  3.64186704e-01  1.91872537e-01  9.98334140e-02\n",
      "    5.17715923e-02  2.68237405e-02  1.38945077e-02  7.19679473e-03\n",
      "    3.72758508e-03  1.93069654e-03 -8.39071512e-01  4.50286061e-01\n",
      "   -8.96541715e-01  1.80309236e-01  7.52012968e-01  9.31325972e-01\n",
      "    9.81419861e-01  9.95004177e-01  9.98658955e-01  9.99640167e-01\n",
      "    9.99903440e-01  9.99974132e-01  9.99993026e-01  9.99998152e-01]\n",
      "  [-9.99990225e-01 -5.52835464e-01  1.89474851e-01  9.99103308e-01\n",
      "    7.11516619e-01  3.98641706e-01  2.10783839e-01  1.09778300e-01\n",
      "    5.69434017e-02  2.95053702e-02  1.52838556e-02  7.91645981e-03\n",
      "    4.10034182e-03  2.12376588e-03  4.42569796e-03  8.33290398e-01\n",
      "   -9.81885552e-01  4.23386246e-02  7.02669263e-01  9.17106748e-01\n",
      "    9.77532685e-01  9.93956089e-01  9.98377383e-01  9.99564648e-01\n",
      "    9.99883175e-01  9.99968648e-01  9.99991596e-01  9.99997735e-01]\n",
      "  [-5.36572933e-01 -6.77637234e-02 -7.75643140e-02  9.95338023e-01\n",
      "    7.60201216e-01  4.32542890e-01  2.29616582e-01  1.19712204e-01\n",
      "    6.21136874e-02  3.21867876e-02  1.66731738e-02  8.63612071e-03\n",
      "    4.47309762e-03  2.31683510e-03  8.43853951e-01  9.97701406e-01\n",
      "   -9.96987343e-01 -9.64481086e-02  6.49687648e-01  9.01613355e-01\n",
      "    9.73281145e-01  9.92808640e-01  9.98069108e-01  9.99481857e-01\n",
      "    9.99861002e-01  9.99962687e-01  9.99989986e-01  9.99997318e-01]\n",
      "  [ 4.20167029e-01  4.35084194e-01 -3.39054674e-01  9.72386658e-01\n",
      "    8.04950118e-01  4.65843111e-01  2.48363748e-01  1.29634142e-01\n",
      "    6.72823042e-02  3.48679759e-02  1.80624593e-02  9.35577694e-03\n",
      "    4.84585296e-03  2.50990433e-03  9.07446802e-01  9.00389791e-01\n",
      "   -9.40766692e-01 -2.33375713e-01  5.93342483e-01  8.84867311e-01\n",
      "    9.68666852e-01  9.91561890e-01  9.97733951e-01  9.99391913e-01\n",
      "    9.99836862e-01  9.99956250e-01  9.99988258e-01  9.99996841e-01]\n",
      "  [ 9.90607381e-01  8.23798418e-01 -5.76289773e-01  9.30691600e-01\n",
      "    8.45531583e-01  4.98496115e-01  2.67018318e-01  1.39543116e-01\n",
      "    7.24491179e-02  3.75489108e-02  1.94517095e-02  1.00754285e-02\n",
      "    5.21860737e-03  2.70297355e-03  1.36737213e-01  5.66882908e-01\n",
      "   -8.17245424e-01 -3.65804791e-01  5.33925474e-01  8.66891921e-01\n",
      "    9.63691473e-01  9.90216017e-01  9.97372091e-01  9.99294817e-01\n",
      "    9.99810815e-01  9.99949217e-01  9.99986410e-01  9.99996364e-01]\n",
      "  [ 6.50287867e-01  9.96409178e-01 -7.72298217e-01  8.71056616e-01\n",
      "    8.81735504e-01  5.30456543e-01  2.85573363e-01  1.49438128e-01\n",
      "    7.76139870e-02  4.02295776e-02  2.08409242e-02  1.07950754e-02\n",
      "    5.59136132e-03  2.89604254e-03 -7.59687901e-01  8.46681297e-02\n",
      "   -6.35260105e-01 -4.91182625e-01  4.71744150e-01  8.47712100e-01\n",
      "    9.58356857e-01  9.88771081e-01  9.96983469e-01  9.99190450e-01\n",
      "    9.99782801e-01  9.99941707e-01  9.99984384e-01  9.99995828e-01]\n",
      "  [-2.87903309e-01  9.07636404e-01 -9.13058043e-01  7.94631183e-01\n",
      "    9.13374484e-01  5.61680019e-01  3.04021984e-01  1.59318209e-01\n",
      "    8.27767700e-02  4.29099537e-02  2.22300962e-02  1.15147159e-02\n",
      "    5.96411480e-03  3.08911153e-03 -9.57659483e-01 -4.19757247e-01\n",
      "   -4.07829583e-01 -6.07092440e-01  4.07120496e-01  8.27354550e-01\n",
      "    9.52665031e-01  9.87227261e-01  9.96568143e-01  9.99078929e-01\n",
      "    9.99752879e-01  9.99933720e-01  9.99982238e-01  9.99995232e-01]\n",
      "  [-9.61397469e-01  5.80767393e-01 -9.88499582e-01  7.02888548e-01\n",
      "    9.40284669e-01  5.92123091e-01  3.22357237e-01  1.69182345e-01\n",
      "    8.79373401e-02  4.55900207e-02  2.36192271e-02  1.22343516e-02\n",
      "    6.33686688e-03  3.28218029e-03 -2.75163352e-01 -8.14069569e-01\n",
      "   -1.51223734e-01 -7.11300015e-01  3.40389103e-01  8.05847526e-01\n",
      "    9.46618080e-01  9.85584795e-01  9.96125996e-01  9.98960257e-01\n",
      "    9.99721050e-01  9.99925137e-01  9.99979913e-01  9.99994636e-01]\n",
      "  [-7.50987232e-01  1.01548195e-01 -9.93225813e-01  5.97597003e-01\n",
      "    9.62326825e-01  6.21743560e-01  3.40572357e-01  1.79029569e-01\n",
      "    9.30955410e-02  4.82697599e-02  2.50083115e-02  1.29539799e-02\n",
      "    6.70961849e-03  3.47524881e-03  6.60316706e-01 -9.94830608e-01\n",
      "    1.16200350e-01 -8.01796615e-01  2.71895438e-01  7.83220887e-01\n",
      "    9.40218329e-01  9.83843684e-01  9.95657206e-01  9.98834312e-01\n",
      "    9.99687254e-01  9.99916077e-01  9.99977469e-01  9.99993980e-01]\n",
      "  [ 1.49877205e-01 -4.04309660e-01 -9.26898658e-01  4.80786294e-01\n",
      "    9.79386747e-01  6.50500178e-01  3.58660549e-01  1.88858896e-01\n",
      "    9.82512534e-02  5.09491526e-02  2.63973475e-02  1.36736017e-02\n",
      "    7.08236871e-03  3.66831757e-03  9.88704622e-01 -9.14622188e-01\n",
      "    3.75311702e-01 -8.76837790e-01  2.01994076e-01  7.59506106e-01\n",
      "    9.33468044e-01  9.82004225e-01  9.95161653e-01  9.98701274e-01\n",
      "    9.99651551e-01  9.99906540e-01  9.99974906e-01  9.99993265e-01]]], shape=(1, 20, 28), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def get_angles(pos, i, d_model):\n",
    "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "  return pos * angle_rates\n",
    "\n",
    "# positional encoding 的產出需要句長與詞深度這兩個參數。\n",
    "def positional_encoding(position, d_model):\n",
    "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "  # apply sin to even indices in the array; 2i\n",
    "  sines = np.sin(angle_rads[:, 0::2])\n",
    "  # apply cos to odd indices in the array; 2i+1\n",
    "  cosines = np.cos(angle_rads[:, 1::2])\n",
    "  pos_encoding = np.concatenate([sines, cosines], axis=-1)\n",
    "  pos_encoding = pos_encoding[np.newaxis, ...]\n",
    "  return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "seq_len = 20 # 20 天資料\n",
    "d_model = 28 # 28 維度資料\n",
    "pos_encoding = positional_encoding(seq_len, d_model)\n",
    "print(pos_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look ahead mask\n",
    "# 建立一個 2 維矩陣，維度為 (size, size)，\n",
    "def create_look_ahead_mask(size):\n",
    "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "  return mask  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "look_ahead_mask tf.Tensor(\n",
      "[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(20, 20), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "seq_len = x_train.shape[1]\n",
    "look_ahead_mask = create_look_ahead_mask(seq_len)\n",
    "print(\"look_ahead_mask\", look_ahead_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"計算注意權重.\n",
    "    mask 可以根據不同的 mask 型態有不同的 shape (padding or look ahead)，\n",
    "    但必須具有 broadcasting 特性.\n",
    "  \n",
    "    Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable \n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "    \n",
    "    Returns:\n",
    "    output, attention_weights\n",
    "    \"\"\"\n",
    "    # 將 `q`、 `k` 做點積再 scale，(transpose_b=True 意思是讓第二個參數 k 先轉置)\n",
    "    # 轉置後相乘我想不必多說，就是 dot product 的觀念。\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "  \n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)  # 取得 seq_k 的序列長度\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)  # scale by sqrt(dk)\n",
    "\n",
    "    # 將遮罩「加」到被丟入 softmax 前的 logits\n",
    "    if mask is not None:\n",
    "        # 經過此步之後，scaled_attention_logits 的 padding 部分會變成一個很大的負數\n",
    "        scaled_attention_logits += (mask * -1e9) \n",
    "\n",
    "    # 取 softmax 是為了得到總和為 1 的比例之後對 `v` 做加權平均\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "  \n",
    "    # 以注意權重對 v 做加權平均（weighted average）\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 實作一個執行多頭注意力機制的 keras layer\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "    # 初始化，建立參數 d_model(詞向量深度)，nm_heads(頭的數量)。\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # 驗證 d_model 是否可以被 num_heads 整除\n",
    "        assert d_model % self.num_heads == 0  \n",
    "        # 每個頭的新詞向量深度\n",
    "        self.depth = d_model // self.num_heads  \n",
    "        \n",
    "        # 提供的 q k v 三個參數的線性轉換。\n",
    "        self.wq = tf.keras.layers.Dense(d_model)  \n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        # 多個 heads 串接之後通過的一次線性轉換\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "  \n",
    "    # 這個就是包裝過後的切頭公式\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"把 x 最後一維切割成 (num_heads, depth).\n",
    "        傳置加重塑後 x 變這樣: (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "  \n",
    "    # 定義這個方法之後主程式宣告完 class 之後就可以直接用呼叫了，這個晚一點會說明。\n",
    "    def __call__(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "        \n",
    "        # 把 q k v 必須都分別做一次線性轉換，老師的影片裡面有說明 q*w, k*w, v*w。\n",
    "        # w 是 AI 可以學習出來的\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "        \n",
    "        # q, k, v 分別切頭\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "        \n",
    "        # 我們前面有實作一個 self-attention 的 func，這個時候就賣上用場了。\n",
    "        # 利用 broadcasting 讓每個句子的每個 head 的 qi, ki, vi 都各自進行注意力機制。\n",
    "        # 還記得前面 padding 產出的 shape 嗎? 一樣是四維的，就是為了呼應這邊切的多頭機制。\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        \n",
    "        # 多頭算完之後記得要合併回來呀 ! 很重要!先 Transpose， seq 又變回第二維度了。\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(scaled_attention, \n",
    "                                    (batch_size, -1, self.d_model)) \n",
    "\n",
    "\n",
    "        # 合併好後通過最後一個線性轉換\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_model: 28\n",
      "num_heads: 2\n",
      "\n",
      "q.shape:  (3571, 20, 28)\n",
      "k.shape:  (3571, 20, 28)\n",
      "v.shape:  (3571, 20, 28)\n",
      "look_ahead_mask.shape:  (20, 20)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "d_model = 28\n",
    "num_heads = 2  # 再次提醒，num_heads 必須要可以整除 d_model\n",
    "\n",
    "print(f\"d_model: {d_model}\")\n",
    "print(f\"num_heads: {num_heads}\\n\")\n",
    "\n",
    "# 初始化一個 multi-head attention layer\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "# q, k, v 都設定維 emb_inp。\n",
    "v = k = q = x_train\n",
    "look_ahead_mask = create_look_ahead_mask(20)\n",
    "print(\"q.shape: \", q.shape)\n",
    "print(\"k.shape: \", k.shape)\n",
    "print(\"v.shape: \", v.shape)\n",
    "print(\"look_ahead_mask.shape: \", look_ahead_mask.shape)  # 注意這邊 mask 是一個 4 維張量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer dense_4 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_5 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_6 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "output.shape:  (3571, 20, 28)\n",
      "attention_weights.shape:  (3571, 2, 20, 20)\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "output:  tf.Tensor(\n",
      "[[[ 1247.0939   -1297.6564    -365.52316  ...    75.72533   -393.79773\n",
      "    -406.74573 ]\n",
      "  [ 1319.7014   -1369.1094    -331.81512  ...    39.361206  -258.39893\n",
      "    -305.5899  ]\n",
      "  [ 1342.3557   -1359.7195    -350.1835   ...   -40.7713    -142.90027\n",
      "    -292.2481  ]\n",
      "  ...\n",
      "  [ 1560.5424   -1346.8423    -515.0421   ...   -24.202347   -28.782211\n",
      "    -212.04922 ]\n",
      "  [ 1560.5424   -1346.8423    -515.0421   ...   -24.202347   -28.782211\n",
      "    -212.04922 ]\n",
      "  [ 1560.5424   -1346.8423    -515.0421   ...   -24.202347   -28.782211\n",
      "    -212.04922 ]]\n",
      "\n",
      " [[ 1319.7014   -1369.1094    -331.81512  ...    39.361206  -258.39893\n",
      "    -305.5899  ]\n",
      "  [ 1342.3557   -1359.7195    -350.1835   ...   -40.7713    -142.90027\n",
      "    -292.2481  ]\n",
      "  [ 1316.9645   -1300.2197    -400.18988  ...   -53.13733   -267.80453\n",
      "    -348.57645 ]\n",
      "  ...\n",
      "  [ 1560.5424   -1346.8423    -515.0421   ...   -24.202347   -28.782211\n",
      "    -212.04922 ]\n",
      "  [ 1560.5424   -1346.8423    -515.0421   ...   -24.202347   -28.782211\n",
      "    -212.04922 ]\n",
      "  [ 1560.5424   -1346.8423    -515.0421   ...   -24.202347   -28.782211\n",
      "    -212.04922 ]]\n",
      "\n",
      " [[ 1342.3557   -1359.7195    -350.1835   ...   -40.7713    -142.90027\n",
      "    -292.2481  ]\n",
      "  [ 1316.9645   -1300.2197    -400.18988  ...   -53.13733   -267.80453\n",
      "    -348.57645 ]\n",
      "  [ 1316.9645   -1300.2197    -400.18988  ...   -53.13733   -267.80453\n",
      "    -348.57645 ]\n",
      "  ...\n",
      "  [ 1560.5424   -1346.8423    -515.0421   ...   -24.202347   -28.782211\n",
      "    -212.04922 ]\n",
      "  [ 1560.5424   -1346.8423    -515.0421   ...   -24.202347   -28.782211\n",
      "    -212.04922 ]\n",
      "  [ 1560.5424   -1346.8423    -515.0421   ...   -24.202347   -28.782211\n",
      "    -212.04922 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 2260.4675   -2236.958     -602.3135   ...   -19.631226  -393.2848\n",
      "    -598.26    ]\n",
      "  [ 2260.4675   -2236.958     -602.3135   ...   -19.631226  -393.2848\n",
      "    -598.26    ]\n",
      "  [ 2260.4675   -2236.958     -602.3135   ...   -19.631226  -393.2848\n",
      "    -598.26    ]\n",
      "  ...\n",
      "  [ 2244.3433   -2187.2449    -683.30237  ...   -48.44812   -482.93\n",
      "    -539.46893 ]\n",
      "  [ 2244.3433   -2187.2449    -683.30237  ...   -48.44812   -482.93\n",
      "    -539.46893 ]\n",
      "  [ 1975.3945   -2098.82      -628.1073   ...  -117.59531   -599.00354\n",
      "    -644.2282  ]]\n",
      "\n",
      " [[ 2240.4631   -2185.9395    -632.99286  ...    55.79138   -503.4785\n",
      "    -640.88635 ]\n",
      "  [ 2240.4631   -2185.9395    -632.99286  ...    55.79138   -503.4785\n",
      "    -640.88635 ]\n",
      "  [ 2240.4631   -2185.9395    -632.99286  ...    55.79138   -503.4785\n",
      "    -640.88635 ]\n",
      "  ...\n",
      "  [ 2244.3433   -2187.2449    -683.30237  ...   -48.44812   -482.93\n",
      "    -539.46893 ]\n",
      "  [ 1975.3945   -2098.82      -628.1073   ...  -117.59531   -599.00354\n",
      "    -644.2282  ]\n",
      "  [ 1975.3945   -2098.82      -628.1073   ...  -117.59531   -599.00354\n",
      "    -644.2282  ]]\n",
      "\n",
      " [[ 2355.2153   -2265.0737    -635.86035  ...   100.93457   -422.2549\n",
      "    -543.69006 ]\n",
      "  [ 2355.2153   -2265.0737    -635.86035  ...   100.93457   -422.2549\n",
      "    -543.69006 ]\n",
      "  [ 2415.372    -2130.0942    -760.9138   ...   243.49088   -575.9021\n",
      "    -568.2035  ]\n",
      "  ...\n",
      "  [ 1975.3945   -2098.82      -628.1073   ...  -117.59531   -599.00354\n",
      "    -644.2282  ]\n",
      "  [ 1975.3945   -2098.82      -628.1073   ...  -117.59531   -599.00354\n",
      "    -644.2282  ]\n",
      "  [ 2124.8782   -2146.143     -702.5679   ...  -129.21182   -503.96423\n",
      "    -581.7883  ]]], shape=(3571, 20, 28), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "output, attention_weights = mha(v, k, q, look_ahead_mask)\n",
    "print(\"output.shape: \", output.shape)\n",
    "# attention_weights.shape 仍然保有多頭的樣子\n",
    "print(\"attention_weights.shape: \", attention_weights.shape)  \n",
    "print(\"---\"*20)\n",
    "print(\"---\"*20)\n",
    "print(\"output: \", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立 Transformer 裡 Encoder / Decoder layer 都有使用到的 Feed Forward 元件\n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "  \n",
    "  # 此 Model 對輸入做兩次線性轉換，中間加了一個 ReLU activation func\n",
    "  return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TheLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(TheLayer, self).__init__()\n",
    "\n",
    "        # 建立 2 個 Sub-Layer mha丶ffn\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        # layer norm 很常在 RNN-based 的模型被使用。一個 sub-layer 配一個 layer norm\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6) # 給一個小的 float 避免算標準差時除以 0。\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        # 一樣，一個 sub-layer 一個 dropout layer\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        \n",
    "        x = self.layernorm1(x)  \n",
    "        \n",
    "        attn_output, attn = self.mha(x, x, x, mask)  \n",
    "        attn_output = self.dropout1(attn_output, training=training) \n",
    "        out1 = self.layernorm2(x + attn_output)  \n",
    "\n",
    "        # sub-layer 2: FFN\n",
    "        # 強調資料關鍵特徵\n",
    "        ffn_output = self.ffn(out1) \n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm3(out1 + ffn_output)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer the_layer is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=770, shape=(3571, 20, 28), dtype=float32, numpy=\n",
       "array([[[ 3.4103537 ,  0.01985091,  0.74918485, ...,  0.58627474,\n",
       "          0.5234207 , -1.314049  ],\n",
       "        [ 3.2908432 , -0.07954286, -0.60534906, ..., -0.8994587 ,\n",
       "          0.9736384 , -0.7727401 ],\n",
       "        [ 3.0684516 , -0.09027258, -0.5664243 , ...,  0.22996233,\n",
       "          0.83399105, -0.9939292 ],\n",
       "        ...,\n",
       "        [ 2.9768589 , -0.03298631, -0.44867444, ...,  0.14257766,\n",
       "          0.6116736 , -0.99784887],\n",
       "        [ 2.9365754 ,  0.14070904, -1.2753508 , ...,  0.23160835,\n",
       "          0.85259664, -1.032717  ],\n",
       "        [ 3.1422853 ,  1.0041803 , -0.6628128 , ...,  0.12204027,\n",
       "          0.7118294 , -1.1834507 ]],\n",
       "\n",
       "       [[ 3.1329968 ,  0.05732667, -0.83186316, ...,  0.08261821,\n",
       "          0.89034736, -0.6229142 ],\n",
       "        [ 3.1012864 , -0.05822286, -0.5505539 , ...,  0.23435576,\n",
       "          0.85639966, -0.81267583],\n",
       "        [ 3.1120343 ,  0.05693801, -0.5416526 , ...,  0.32935756,\n",
       "          0.5624053 , -1.0190328 ],\n",
       "        ...,\n",
       "        [ 3.1182904 ,  0.16717173, -1.0771933 , ...,  0.37951806,\n",
       "          0.90728   , -0.58405703],\n",
       "        [ 3.0108235 ,  0.3644572 ,  0.7620322 , ...,  0.9165201 ,\n",
       "          0.6105999 , -0.7458564 ],\n",
       "        [ 3.0408525 ,  1.1564449 , -0.4110372 , ...,  0.41159782,\n",
       "          0.2149666 , -1.2111262 ]],\n",
       "\n",
       "       [[ 2.991412  , -1.2086489 , -0.54186386, ...,  0.32386336,\n",
       "          0.99419117, -1.2366196 ],\n",
       "        [ 3.2395952 , -0.12205744, -0.42166418, ...,  0.32435486,\n",
       "          1.044589  , -0.7276358 ],\n",
       "        [ 3.0830848 ,  0.9877106 , -0.40662432, ...,  0.33942676,\n",
       "          0.9435122 , -0.9151506 ],\n",
       "        ...,\n",
       "        [ 3.1411927 ,  1.1212922 , -0.23176497, ...,  0.44697583,\n",
       "          0.90284973, -1.2095006 ],\n",
       "        [ 3.2305286 , -0.12634355, -0.35986874, ...,  0.32195076,\n",
       "          0.8643162 , -1.0146987 ],\n",
       "        [ 3.0607553 , -0.10812695, -0.6094041 , ...,  0.09077674,\n",
       "          0.7494764 , -0.9310038 ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 3.0112746 , -0.03001579, -0.56660473, ...,  0.27911726,\n",
       "          0.5495883 , -0.97746414],\n",
       "        [ 3.1595974 , -0.09943923, -0.447331  , ...,  0.35377795,\n",
       "          0.816575  , -0.9323344 ],\n",
       "        [ 3.2312026 , -0.30048394, -0.4392911 , ..., -0.9957663 ,\n",
       "          0.8495255 , -1.2300807 ],\n",
       "        ...,\n",
       "        [ 3.024111  , -0.06532328, -0.5564762 , ...,  0.24443288,\n",
       "          0.81717944, -0.8478466 ],\n",
       "        [ 3.051101  , -0.07122164, -0.6025336 , ...,  0.23360313,\n",
       "          0.7864568 , -0.82574683],\n",
       "        [ 3.043203  , -0.9152519 , -0.6707434 , ...,  0.18671532,\n",
       "          0.867266  , -0.8175681 ]],\n",
       "\n",
       "       [[ 3.1171737 , -0.03859049, -0.5364078 , ...,  0.30698028,\n",
       "          0.8778393 , -0.9286857 ],\n",
       "        [ 3.0176682 , -0.15715867, -0.66194856, ...,  0.38412344,\n",
       "          0.80586946, -0.85433054],\n",
       "        [ 3.2978663 , -0.10606188, -0.51293236, ...,  0.25949627,\n",
       "          0.27854776, -0.7482501 ],\n",
       "        ...,\n",
       "        [ 3.1988442 , -0.04480279, -0.6506587 , ...,  0.1347025 ,\n",
       "          0.64329636, -0.6969759 ],\n",
       "        [ 3.0795197 ,  0.15315585, -0.69724673, ...,  0.34769621,\n",
       "          0.8528288 , -0.6007637 ],\n",
       "        [ 3.3906848 ,  0.22991079, -0.41127932, ...,  0.35792005,\n",
       "          0.808399  , -0.48292413]],\n",
       "\n",
       "       [[ 3.0589533 , -0.01201892, -0.5657989 , ...,  0.27254093,\n",
       "          0.8088379 , -0.93135417],\n",
       "        [ 3.3891287 , -0.24392128, -0.49053875, ...,  0.20432669,\n",
       "          1.0151964 , -0.8184756 ],\n",
       "        [ 2.8628933 ,  0.31242394, -1.0186243 , ...,  1.1166044 ,\n",
       "          1.1737394 , -0.7470494 ],\n",
       "        ...,\n",
       "        [ 3.045778  ,  0.09185205, -1.2583872 , ...,  0.10486498,\n",
       "          0.78908986, -0.72355914],\n",
       "        [ 2.9799871 ,  1.0160786 , -0.5539835 , ...,  0.26595753,\n",
       "          0.79687345, -0.8292763 ],\n",
       "        [ 3.4316719 , -0.00645722, -0.55889934, ...,  0.39530236,\n",
       "          0.9354743 , -0.47550112]]], dtype=float32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_layer = TheLayer(28, 2, 112)\n",
    "out = the_layer(x_train, True, look_ahead_mask)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParentLayer(tf.keras.layers.Layer):\n",
    "  # 初始化參數 : \n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, rate=0.1):\n",
    "        super(ParentLayer, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.pos_encoding = positional_encoding(20, self.d_model)\n",
    "\n",
    "        # 建立 `num_layers` 個 EncoderLayers\n",
    "        self.enc_layers = [TheLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "        # 建立一個 Dropout\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, training, mask):\n",
    "        # 輸入的 x.shape == (batch_size, input_seq_len)\n",
    "        input_seq_len = tf.shape(x)[1]\n",
    "\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))  # 並依照論文乘上 sqrt(d_model)\n",
    "        x += self.pos_encoding[:, :input_seq_len, :] # 再加上對應長度的位置編碼\n",
    "\n",
    "        # 對 embedding 跟位置編碼的總合做 regularization\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i, enc_layer in enumerate(self.enc_layers):\n",
    "          x = enc_layer(x, training, mask)\n",
    "\n",
    "        return x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer parent_layer is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "parent_out: tf.Tensor(\n",
      "[[[ 2.8746946   1.633085    0.6185652  ...  0.8669781   0.7489447\n",
      "   -1.7548804 ]\n",
      "  [ 2.7052305   0.91807085  0.77326584 ...  0.88077307  0.6397235\n",
      "   -1.604751  ]\n",
      "  [ 2.6815305   0.9202407   0.78430384 ...  0.91225845  0.6332881\n",
      "   -1.5919785 ]\n",
      "  ...\n",
      "  [ 3.20774     1.0407766   0.85505474 ...  0.39874393  0.21156798\n",
      "   -1.4060303 ]\n",
      "  [ 3.2708867   0.92264533  0.9258672  ...  0.81493974  0.09914461\n",
      "   -1.540654  ]\n",
      "  [ 3.209761    0.9063616   0.8277502  ...  0.97886366  0.15030946\n",
      "   -1.3752272 ]]\n",
      "\n",
      " [[ 2.6172798   0.84637177  0.7646651  ...  0.9550586   0.50934225\n",
      "   -1.546025  ]\n",
      "  [ 2.3273442   1.041599    0.797919   ...  1.0015316   0.64235353\n",
      "   -1.6591523 ]\n",
      "  [ 2.632501    0.92437726  0.8200277  ...  0.8490933   0.69821024\n",
      "   -1.6331428 ]\n",
      "  ...\n",
      "  [ 3.647423    1.4745796   0.96859956 ...  0.05953818 -0.09363736\n",
      "   -0.5492972 ]\n",
      "  [-0.44384935  0.666051    1.7210286  ...  0.5749117   0.25241172\n",
      "   -2.6317215 ]\n",
      "  [ 3.6565912   0.98649555  0.662234   ...  0.21086429 -0.03610245\n",
      "   -1.3348914 ]]\n",
      "\n",
      " [[ 3.1072545   1.4955933   0.6749165  ...  0.85579824  0.45481476\n",
      "   -1.4113677 ]\n",
      "  [ 2.8984442   1.2497816   0.7403458  ...  1.0177579   0.64399606\n",
      "   -1.8360593 ]\n",
      "  [ 2.9383092  -0.5783275   1.3251188  ...  0.8636114   1.0333292\n",
      "   -1.0130923 ]\n",
      "  ...\n",
      "  [ 2.9753592   1.0587038   0.8207384  ...  0.8371835  -0.14329822\n",
      "   -1.4470158 ]\n",
      "  [ 3.1157303   1.0253769   0.79602003 ...  0.25462663  0.4306593\n",
      "   -1.7300975 ]\n",
      "  [ 2.9234736   0.8825167   0.850475   ...  0.2793944   0.38099274\n",
      "   -1.6175276 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 2.505383    1.2470325   0.7297446  ...  0.9461148   0.61480325\n",
      "   -0.7691172 ]\n",
      "  [ 2.6568446   0.9303486   0.68274355 ...  0.94898117  0.617142\n",
      "   -1.6434627 ]\n",
      "  [ 2.9267921  -0.5182876   1.1060745  ...  0.9547323   0.52729213\n",
      "   -1.461805  ]\n",
      "  ...\n",
      "  [ 3.1905994   0.83718383  0.00559408 ...  0.82837284  0.4956199\n",
      "   -1.554593  ]\n",
      "  [-0.13896386  1.245106    1.5712798  ... -0.0561906   1.2466738\n",
      "   -1.0391368 ]\n",
      "  [ 3.4909115   0.8077732   0.03951326 ...  0.8337994   0.25944164\n",
      "   -1.5584613 ]]\n",
      "\n",
      " [[ 2.944427    1.175849    0.9099664  ...  0.8659225  -0.21848653\n",
      "   -1.5883433 ]\n",
      "  [ 2.8549411   1.0962843   0.77466136 ...  0.9779077  -0.08214836\n",
      "   -1.3095503 ]\n",
      "  [ 2.6282551   0.90524894  0.658319   ...  0.8455726   0.63515365\n",
      "   -1.4827402 ]\n",
      "  ...\n",
      "  [ 1.0076418   2.1156554   2.135188   ... -0.49399024 -0.68700933\n",
      "   -0.23108731]\n",
      "  [ 3.8951979  -0.7227313   0.6057122  ...  0.43792388  0.28049016\n",
      "   -1.0190295 ]\n",
      "  [ 0.52947354  1.1353245   1.707267   ...  0.5596074   0.26044244\n",
      "   -2.1078944 ]]\n",
      "\n",
      " [[ 2.6827738   1.6422272   0.7185753  ...  0.86837775  0.6827456\n",
      "   -1.5862921 ]\n",
      "  [ 3.4350152  -0.20411131  1.0095471  ...  0.6783777   0.9022196\n",
      "   -1.2910594 ]\n",
      "  [ 2.696763    0.95640373  0.57807416 ...  0.79567564  0.75255877\n",
      "   -1.5188632 ]\n",
      "  ...\n",
      "  [ 2.9646864   1.0375412   0.7829186  ...  0.8815229   0.50349414\n",
      "   -1.4280685 ]\n",
      "  [ 3.4468668   1.217961   -0.06937782 ...  0.91407096  0.39445668\n",
      "   -0.7732398 ]\n",
      "  [ 2.853984    0.94458485  0.83863187 ...  0.8695283   0.5378553\n",
      "   -1.4751744 ]]], shape=(3571, 20, 28), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "num_layers = 1 # 2 層的 Encoder\n",
    "d_model = 28 # 詞向量深度\n",
    "num_heads = 2 # 切 2 頭\n",
    "dff = 112  # FFN 神經元個數\n",
    "\n",
    "# 初始化一個 Encoder\n",
    "parent_layer = ParentLayer(num_layers, d_model, num_heads, dff)\n",
    "\n",
    "# 將 2 維的索引序列丟入 Encoder 做編碼\n",
    "parent_out = parent_layer(x_train, training=True, mask=look_ahead_mask) # 這邊關閉 dropout，不使用 mask\n",
    "print(\"parent_out:\", parent_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigMama(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, rate=0.1):\n",
    "        super(BigMama, self).__init__()\n",
    "\n",
    "        self.parent_layer = ParentLayer(num_layers, d_model, num_heads, dff, rate)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "  \n",
    "  \n",
    "    def call(self, inp, training, mask):\n",
    "\n",
    "        output = self.parent_layer(inp, training, mask)  \n",
    "\n",
    "        # final_output.shape == (batch_size, tar_seq_len, 1)\n",
    "        final_output = self.final_layer(output)  \n",
    "        \n",
    "        final_output = final_output[:, -1, :]\n",
    "        \n",
    "        return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer big_mama_5 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "predictions: tf.Tensor(\n",
      "[[0.53099674]\n",
      " [0.4591116 ]\n",
      " [0.6237141 ]\n",
      " ...\n",
      " [0.73760617]\n",
      " [0.50679874]\n",
      " [0.6166745 ]], shape=(3571, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 超參數\n",
    "num_layers = 1\n",
    "d_model = 28\n",
    "num_heads = 2\n",
    "dff = 112\n",
    "\n",
    "\n",
    "# 建立 transformer\n",
    "mama = BigMama(num_layers, d_model, num_heads, dff)\n",
    "\n",
    "# 將英文、中文序列丟入取得 Transformer 預測下個中文字的結果\n",
    "predictions, attn_weights = mama(x_train, True, look_ahead_mask)\n",
    "\n",
    "print(\"predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "  # 這次的 mask 將 real 序列中不等於 0 的位置視為 1，其餘為 0 \n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  # 照樣計算所有位置的 cross entropy 但不加總\n",
    "  loss_ = loss_object(real, pred)\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype) # 統一型別\n",
    "  loss_ *= mask  # 只計算非 <pad> 位置的損失 \n",
    "  \n",
    "  return tf.reduce_mean(loss_) # 計算 loss 張量所有數字的平均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  # 論文預設 `warmup_steps` = 4000，4000 個 step 之後開始降低 learning_rates\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "    self.d_model = tf.cast(d_model, tf.float32)\n",
    "    self.warmup_steps = warmup_steps\n",
    "    \n",
    "  def __call__(self, step):\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "    \n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "  \n",
    "# 將客製化 learning rate schdeule 丟入 Adam opt.\n",
    "# Adam opt. 的參數都跟論文相同\n",
    "learning_rate = CustomSchedule(d_model)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10  # 64 筆資料綁成一批\n",
    "BUFFER_SIZE = 100\n",
    "\n",
    "num_layers = 1  # 幾層\n",
    "d_model =28  # 詞向量深度\n",
    "dff = 112  # FFN 神經元個數\n",
    "num_heads = 4  # multi-head 個數\n",
    "dropout_rate = 0.1\n",
    "EPOCHS = 20 # 訓練週期"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available:  True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 讓 numpy 不要顯示科學記號\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "# GPU 設定\n",
    "print(\"GPU Available: \", tf.test.is_gpu_available())\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# 定義一些之後在儲存檔案時會用到的路徑變數\n",
    "output_dir = \"nmt\"\n",
    "checkpoint_path = os.path.join(output_dir, \"checkpoints\")\n",
    "log_dir = os.path.join(output_dir, 'logs')\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction='none')\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss') # 看 log 需要\n",
    "train_accuracy = tf.keras.metrics.BinaryAccuracy(name='train_accuracy') # 看 log 需要\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    loss_ = loss_object(real, pred)\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "這個 mama 有 1 層\n",
      "d_model: 28\n",
      "num_heads: 4\n",
      "dff: 112\n",
      "dropout_rate: 0.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 將客製化 learning rate schdeule 丟入 Adam opt.\n",
    "# Adam opt. 的參數都跟論文相同\n",
    "learning_rate = CustomSchedule(d_model)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "# 實際訓練以及及時存檔\n",
    "mama = BigMama(num_layers, d_model, num_heads, dff)\n",
    "\n",
    "print(f\"\"\"這個 mama 有 {num_layers} 層\n",
    "d_model: {d_model}\n",
    "num_heads: {num_heads}\n",
    "dff: {dff}\n",
    "dropout_rate: {dropout_rate}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "沒找到 checkpoint，從頭訓練。\n"
     ]
    }
   ],
   "source": [
    "train_perc = 90  # 用 90% 資料訓練\n",
    "\n",
    "# 方便比較不同實驗/ 不同超參數設定的結果\n",
    "run_id = f\"{num_layers}layers_{d_model}d_{num_heads}heads_{dff}dff_{train_perc}train_perc\"\n",
    "checkpoint_path = os.path.join(checkpoint_path, run_id)\n",
    "log_dir = os.path.join(log_dir, run_id)\n",
    "\n",
    "# tf.train.Checkpoint 可以幫我們把想要存下來的東西整合起來，方便儲存與讀取\n",
    "# 一般來說會存 transformer 以及 optimizer 的狀態\n",
    "ckpt = tf.train.Checkpoint(model=mama, optimizer=optimizer)\n",
    "\n",
    "# ckpt_manager 會去 checkpoint_path 看有沒有符合 ckpt 裡頭定義的東西\n",
    "# 存檔的時候只保留最近 5 次 checkpoints，其他自動刪除\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "\n",
    "# 如果在 checkpoint 路徑上有發現檔案就讀進來\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "\n",
    "    # 用來確認之前訓練多少 epochs 了\n",
    "    last_epoch = int(ckpt_manager.latest_checkpoint.split(\"-\")[-1])\n",
    "    print(f'已讀取最新的 checkpoint，模型已訓練 {last_epoch} epochs。')\n",
    "else:\n",
    "    last_epoch = 0\n",
    "    print(\"沒找到 checkpoint，從頭訓練。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.float32),\n",
    "    tf.TensorSpec(shape=(None), dtype=tf.float32),\n",
    "]\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step(inp, label):\n",
    "\n",
    "    with tf.GradientTape() as tape:  # GradientTape 紀錄數據轉換計算 loss\n",
    "        predictions, _ = mama(inp, True, look_ahead_mask)\n",
    "        print(\"predictions: \", predictions, \" label: \", label)\n",
    "        loss = loss_function(label, predictions)\n",
    "\n",
    "        gradients = tape.gradient(loss, mama.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, mama.trainable_variables))  # 將梯度取出並用 optimizer 隊訓練權重做升降\n",
    "\n",
    "        train_loss(loss)\n",
    "        train_accuracy(label, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "此超參數組合的 Transformer 已經訓練 0 epochs。\n",
      "剩餘 epochs：-20\n",
      "predictions:  Tensor(\"big_mama_7/strided_slice:0\", shape=(None, 1), dtype=float32)  label:  Tensor(\"label:0\", dtype=float32)\n",
      "predictions:  Tensor(\"big_mama_7/strided_slice:0\", shape=(None, 1), dtype=float32)  label:  Tensor(\"label:0\", dtype=float32)\n",
      "Saving checkpoint for epoch 1 at nmt\\checkpoints\\1layers_28d_4heads_112dff_90train_perc\\1layers_28d_4heads_112dff_90train_perc\\ckpt-1\n",
      "Epoch 1 Loss 0.6965 Accuracy 0.4562\n",
      "Time taken for 1 epoch: 18.641513109207153 secs\n",
      "\n",
      "Saving checkpoint for epoch 2 at nmt\\checkpoints\\1layers_28d_4heads_112dff_90train_perc\\1layers_28d_4heads_112dff_90train_perc\\ckpt-2\n",
      "Epoch 2 Loss 0.6930 Accuracy 0.4598\n",
      "Time taken for 1 epoch: 17.354198455810547 secs\n",
      "\n",
      "Saving checkpoint for epoch 3 at nmt\\checkpoints\\1layers_28d_4heads_112dff_90train_perc\\1layers_28d_4heads_112dff_90train_perc\\ckpt-3\n",
      "Epoch 3 Loss 0.6917 Accuracy 0.4584\n",
      "Time taken for 1 epoch: 17.06368350982666 secs\n",
      "\n",
      "Saving checkpoint for epoch 4 at nmt\\checkpoints\\1layers_28d_4heads_112dff_90train_perc\\1layers_28d_4heads_112dff_90train_perc\\ckpt-4\n",
      "Epoch 4 Loss 0.6911 Accuracy 0.4584\n",
      "Time taken for 1 epoch: 16.752596139907837 secs\n",
      "\n",
      "Saving checkpoint for epoch 5 at nmt\\checkpoints\\1layers_28d_4heads_112dff_90train_perc\\1layers_28d_4heads_112dff_90train_perc\\ckpt-5\n",
      "Epoch 5 Loss 0.6914 Accuracy 0.4584\n",
      "Time taken for 1 epoch: 17.285618543624878 secs\n",
      "\n",
      "Saving checkpoint for epoch 6 at nmt\\checkpoints\\1layers_28d_4heads_112dff_90train_perc\\1layers_28d_4heads_112dff_90train_perc\\ckpt-6\n",
      "Epoch 6 Loss 0.6912 Accuracy 0.4584\n",
      "Time taken for 1 epoch: 17.379559993743896 secs\n",
      "\n",
      "Saving checkpoint for epoch 7 at nmt\\checkpoints\\1layers_28d_4heads_112dff_90train_perc\\1layers_28d_4heads_112dff_90train_perc\\ckpt-7\n",
      "Epoch 7 Loss 0.6909 Accuracy 0.4584\n",
      "Time taken for 1 epoch: 17.4751193523407 secs\n",
      "\n",
      "Saving checkpoint for epoch 8 at nmt\\checkpoints\\1layers_28d_4heads_112dff_90train_perc\\1layers_28d_4heads_112dff_90train_perc\\ckpt-8\n",
      "Epoch 8 Loss 0.6907 Accuracy 0.4584\n",
      "Time taken for 1 epoch: 17.304394245147705 secs\n",
      "\n",
      "Saving checkpoint for epoch 9 at nmt\\checkpoints\\1layers_28d_4heads_112dff_90train_perc\\1layers_28d_4heads_112dff_90train_perc\\ckpt-9\n",
      "Epoch 9 Loss 0.6907 Accuracy 0.4584\n",
      "Time taken for 1 epoch: 17.509827375411987 secs\n",
      "\n",
      "Saving checkpoint for epoch 10 at nmt\\checkpoints\\1layers_28d_4heads_112dff_90train_perc\\1layers_28d_4heads_112dff_90train_perc\\ckpt-10\n",
      "Epoch 10 Loss 0.6907 Accuracy 0.4584\n",
      "Time taken for 1 epoch: 17.342994928359985 secs\n",
      "\n",
      "Saving checkpoint for epoch 11 at nmt\\checkpoints\\1layers_28d_4heads_112dff_90train_perc\\1layers_28d_4heads_112dff_90train_perc\\ckpt-11\n",
      "Epoch 11 Loss 0.6906 Accuracy 0.4584\n",
      "Time taken for 1 epoch: 17.608627796173096 secs\n",
      "\n",
      "Saving checkpoint for epoch 12 at nmt\\checkpoints\\1layers_28d_4heads_112dff_90train_perc\\1layers_28d_4heads_112dff_90train_perc\\ckpt-12\n",
      "Epoch 12 Loss 0.6907 Accuracy 0.4584\n",
      "Time taken for 1 epoch: 17.71224594116211 secs\n",
      "\n",
      "Saving checkpoint for epoch 13 at nmt\\checkpoints\\1layers_28d_4heads_112dff_90train_perc\\1layers_28d_4heads_112dff_90train_perc\\ckpt-13\n",
      "Epoch 13 Loss 0.6905 Accuracy 0.4584\n",
      "Time taken for 1 epoch: 17.53785490989685 secs\n",
      "\n",
      "Saving checkpoint for epoch 14 at nmt\\checkpoints\\1layers_28d_4heads_112dff_90train_perc\\1layers_28d_4heads_112dff_90train_perc\\ckpt-14\n",
      "Epoch 14 Loss 0.6906 Accuracy 0.4584\n",
      "Time taken for 1 epoch: 17.515789270401 secs\n",
      "\n",
      "Saving checkpoint for epoch 15 at nmt\\checkpoints\\1layers_28d_4heads_112dff_90train_perc\\1layers_28d_4heads_112dff_90train_perc\\ckpt-15\n",
      "Epoch 15 Loss 0.6905 Accuracy 0.4584\n",
      "Time taken for 1 epoch: 17.584412574768066 secs\n",
      "\n",
      "Saving checkpoint for epoch 16 at nmt\\checkpoints\\1layers_28d_4heads_112dff_90train_perc\\1layers_28d_4heads_112dff_90train_perc\\ckpt-16\n",
      "Epoch 16 Loss 0.6904 Accuracy 0.4584\n",
      "Time taken for 1 epoch: 17.303438425064087 secs\n",
      "\n",
      "Saving checkpoint for epoch 17 at nmt\\checkpoints\\1layers_28d_4heads_112dff_90train_perc\\1layers_28d_4heads_112dff_90train_perc\\ckpt-17\n",
      "Epoch 17 Loss 0.6905 Accuracy 0.4584\n",
      "Time taken for 1 epoch: 17.472243309020996 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "print(f\"此超參數組合的 Transformer 已經訓練 {last_epoch} epochs。\")\n",
    "print(f\"剩餘 epochs：{min(0, last_epoch - EPOCHS)}\")\n",
    "# 用來寫資訊到 TensorBoard，非必要。\n",
    "summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "\n",
    "# 比對設定的 `EPOCHS` 以及已訓練的 `last_epoch` 來決定還要訓練多少 epochs\n",
    "for epoch in range(last_epoch, EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    # 重置紀錄 TensorBoard 的 metrics\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "\n",
    "    # 一個 epoch 就是把我們定義的訓練資料集一個一個 batch 拿出來處理，直到看完整個數據集\n",
    "    for (step_idx, inp) in enumerate(x_train):\n",
    "        # 每次 step 就是將數據丟入 Transformer，讓它生預測結果並計算梯度最小化 loss\n",
    "        \n",
    "        train_step(inp, y_train[step_idx])\n",
    "\n",
    "        # 每個 epoch 完成就存一次檔\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print('Saving checkpoint for epoch {} at {}'.format(epoch + 1, ckpt_save_path))\n",
    "\n",
    "    # 將 loss 以及 accuracy 寫到 TensorBoard 上\n",
    "    with summary_writer.as_default():\n",
    "        tf.summary.scalar(\"train_loss\", train_loss.result(), step=epoch + 1)\n",
    "        tf.summary.scalar(\"train_acc\", train_accuracy.result(), step=epoch + 1)\n",
    "\n",
    "    print('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, train_loss.result(), train_accuracy.result()))\n",
    "    print('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))\n",
    "\n",
    "print(\"訓練已完成，可以進行測試。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
